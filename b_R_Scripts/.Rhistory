colnames(transpose) <- colnames(t(groups[[lm]]))
scale <- scale(transpose, center = FALSE, scale = TRUE) # All the model will use scale data
# Validation data set:
val_transpose <- matrix(as.numeric(t(val_groups[[lm]])),nrow=nrow(t(val_groups[[lm]]))) # Transpose and create matrix.
rownames(val_transpose) <- rownames(t(val_groups[[lm]]))
colnames(val_transpose) <- colnames(t(val_groups[[lm]]))
val_scale <- scale(val_transpose, center = FALSE, scale = TRUE) # All the model will use scale data
# Support Vector Machine Model:
support_vm <- cfBuild(scale, response_matrix, bootNum = 65, ensNum = 65, cpus = 4) # Creates model
# Validation of the models:
pred_val <- cfPredict(support_vm, val_scale)
names(pred_val) <- c("prediction")
mcc_val = mcc(preds = pred_val$prediction, actuals = val_response$responses)
average_right <- data.frame(groups = names(groups)[[lm]],
percentage_accuracy = getAvgAcc(support_vm)$Test,
MCC= mcc_val) # Save %CC value.
accuracy_table <- rbind(accuracy_table, average_right) # Append %CC and MCC to the accuracy table.
# Save model as a R object.
saveRDS(support_vm,
file = paste(output, "1_classyfire_(SVM_models)/1_SVM_",
names(groups)[[lm]], ".R", sep = ""),
ascii = FALSE, version = NULL, compress = TRUE, refhook = NULL)
}
else { next }
}
# OUTPUT:
# Besides the functional models, the OUTPUT of this script is a table with Accuracy score of the model and the
# validation of the model based on the MCC value.
write.table(accuracy_table,
file = paste(output, "1_accuracy_table.txt", sep = ""),
sep = "\t",
quote = FALSE,
row.names = FALSE)
#------------------------------------------ CLASSYFIRE LIPIDOMIC DATA --------------------------------------------#
# Rheumatoid arthritis is a inflammatory disease that characterized for not having a resolution phase in the
# inflammatory procces. One of the most used treatments is modifying antirheumatic drugs (DMARDs), that is not
# always efective.
# We have a dataset of the lipid profiles of rheumatoid arthritis patients that respond or not to the MTX treatment.
# This is the first approach to create a model using machine learning that predicts if a patient will react or not
# to the treatment with DMARDs.
#---> LIBRARY LOAD:
library(classyfire)
library("mltools")
#---> INPUT AND OUTPUT:
# In this section please specify where are the input files and where you want to save the output files.
# In the input and output variable you can see what is the path expected from the user to write.
input <- "C:/Users/hhy270/Documents/GitHub/2018_Machine_Learning_MTX_treatment_in_RA_patients/a_Toy_Data/1_classyfire_(SVM_models)/"
output <- "C:/Users/hhy270/Documents/GitHub/2018_Machine_Learning_MTX_treatment_in_RA_patients/c_Expected_Output/1_classyfire_(SVM_models)/"
# !!!! IMPORTANT: For this script to work the training dataset has to be called: 1_classyfire_(SVM_models)_toy_data.txt
# !!!! IMPORTANT: For this script to work the validation dataset has to be called: 2_classyfire_(SVM_models)_data_validation.txt
#---> DATA MANIPULATION:
# TRAINING SET:
# Data uses to create the model!
# Open the txt file with the profiles information. Make sure that the path is correct:
# The dataset consist in a tab-delimited file in .txt format with the follow specifications:
# Columns: The different samples (each patient data)
# Row number 1: Class row that contains the word "Responder" if the patient respond to treatment and "Non_Responder"
# otherwhise.
# Following rows: All the lipid mediators used to create the model.
# See a_Toy_Data/1_classyfire_(SVM_models)/1_classyfire_(SVM_models)_toy_data.txt
lm_profiles <- read.table(
file = paste(input, "1_classyfire_(SVM_models)_toy_data.txt", sep = ""),
header = TRUE,
row.names = 1,
sep = "\t")
# Separates the profiles data by lipid mediators types.
# by Substrates:
dha <- lm_profiles[2:25, ]
n_three_DPA <- lm_profiles[26:35, ]
epa <- lm_profiles[36:38, ]
aa <- lm_profiles[39:56, ]
# VALIDATION SET:
# Data use to test the model (independent cohort)!
# Open the txt file with the profiles information. Make sure that the path is correct:
# The dataset consist in a tab-delimited file in .txt format with the follow specifications:
# Columns: The different samples (each patient data)
# Row number 1: Class row that contains the word "Responder" if the patient respond to treatment and "Non_Responder"
# otherwhise.
# Following rows: All the lipid mediators used to create the model (NOTE: They have to be in the same order as
# the training dataset).
# See a_Toy_Data/1_classyfire_(SVM_models)/2_classyfire_(SVM_models)_data_validation.txt
val_lm_profiles <- read.table(
file = paste(input, "2_classyfire_(SVM_models)_data_validation.txt", sep = ""),
header = TRUE,
row.names = 1,
sep = "\t")
# by Substrates:
val_dha <- val_lm_profiles[2:25, ]
val_n_three_DPA <- val_lm_profiles[26:35, ]
val_epa <- val_lm_profiles[36:38, ]
val_aa <- val_lm_profiles[39:56, ]
#---> DATA PREPARATION TRAINING DATA:
# Getting the explanatory (x) and response (y) variable. By explanatory, it means all the data that can explain
# why a patient respond or not to the treatment (the lipid meadiator profiles), and the response variable is if
# the patients respond or not to the treatment.
# Transpose columns and rows:
lm_profiles_transpose <- t(lm_profiles)
# Explanatory and Response variable:
response <- data.frame(row.names = row.names(lm_profiles_transpose), # Create the Response variable.
responses = lm_profiles_transpose[, 1])
response_matrix <- as.matrix(response)
explanatorys <-  lm_profiles_transpose[, -1] # Delete the first row that contains the Response variable.
# Because the response variable was in the data.frame all the elements were saved as factors, and classyfire
# requieres a numeric matrix. here we make it:
explanatory <- matrix(as.numeric(unlist(explanatorys)),nrow=nrow(explanatorys))
rownames(explanatory) <- rownames(explanatorys)
colnames(explanatory) <- colnames(explanatorys)
# If you are working with machine learning, the best method of scalation is standarization.
# Scale data prevents that the modelfrom being based on variables with high normal values.
explanatory_scale <- scale(explanatory, center = FALSE, scale = TRUE)
#---> DATA PREPARATION VALIDATION DATA:
# Similar process to the training dataset.
# Transpose columns and rows:
val_lm_profiles_transpose <- t(val_lm_profiles)
# Explanatory and Response variable:
val_response <- data.frame(row.names = row.names(val_lm_profiles_transpose), # Create the Response variable.
responses = val_lm_profiles_transpose[, 1])
val_explanatorys <-  val_lm_profiles_transpose[, -1] # Delete the first row that contains the Response variable.
# Because the response variable was in the data.frame all the elements were saved as factors, and classyfire
# requieres a numeric matrix. here we make it:
val_explanatory <- matrix(as.numeric(unlist(val_explanatorys)),nrow=nrow(val_explanatorys))
rownames(val_explanatory) <- rownames(val_explanatorys)
colnames(val_explanatory) <- colnames(val_explanatorys)
# Since  the model is created using scalation and transpose data, the validation datasets has to be scaled and
# transpose as well.
validation_scale <- scale(val_explanatory, center = FALSE, scale = TRUE)
#---> MACHINE LEARNING (Classyfire R):
# Classyfire uses Support Vector Machine to create the Machine Learning Model. SVM classifies, makes a reggresion,
# and creates a novelty detection for the creation of the model.
# The idea is to create several models and see which one fits the best. The models will be based on the whole
# lipid profiles and the different groups based on substrates.
# "cfBuild" to create the SVM:
support_lmprofiles_scale <- cfBuild(explanatory_scale, response_matrix,
bootNum = 70,ensNum = 70, cpus = 4)
# Expositional figures to get info of the model:
ggClassPred(support_lmprofiles_scale, displayAll = TRUE, fillBrewer = TRUE, showText = TRUE)
ggEnsTrend(support_lmprofiles_scale, ylims = c(50,100))
# Model accuracy:
getAvgAcc(support_lmprofiles_scale)$Test # Get the %CC (Overall percentage of correctly classified test objects)
getConfMatr(support_lmprofiles_scale) # Get a table of the consensus classification of the best model.
#---> MODEL VALIDATION:
# "cfPredict" takes the created models and the validation dataset to try to predict which samples belongs to the
# responder and non-responder. It creates a data frame with the identifications and % of accuracy.
prediction_validation <- cfPredict(support_lmprofiles_scale, validation_scale)
names(prediction_validation) <- c("prediction")
# In order to further evaluate the predictivenss of this approach we next calculated  Matthews correlation
# coefficient (MCC), which represents the accuracy of the model at predicting outcome. Very helpful when you
# have imbalance data.
mcc_value = mcc(preds = prediction_validation$prediction, actuals = val_response$responses) # From the mltools package.
# Creates a table with all the models, the %CC and the MCC.
accuracy_table <- data.frame(groups = "scalated lm profiles",
percentage_accuracy = getAvgAcc(support_lmprofiles_scale)$Test,
MCC = mcc_value,
stringsAsFactors = FALSE)
# Save the models as an R object:
# To avoid running all the scripts to obtain the model, it can be saved as R objects. If you want to use it,
# you can call it using "readRDS".
# Make sure that the you specify the path were you want to save your model:
saveRDS(support_lmprofiles_scale,
file = paste(output, "1_SVM_lmprofiles_scale.R",sep = ""),
ascii = FALSE, version = NULL, compress = TRUE, refhook = NULL)
#---> SVM PER GROUP:
# Run the same analysis but automatically for the rest of the groups by Substrates.
# Create a list with the names of all the subgroups:
# Modifying the original lipid mediator file, including other data (such as clinical scores) and dividing the table
# in different sections is possible to create automatically all the models you want. YOU NEED TO MAKE SURE WHATSOEVER
# that the "groups" list and the names of the "groups" list is updated.
groups <- list(dha, n_three_DPA, epa, aa) # Update in case you want to create other models.
val_groups <- list(val_dha, val_n_three_DPA, val_epa, val_aa) # Update in case you want to create other models.
# Create a vector with the names associated to all the elements in the list:
names(groups) <- c("DHA", "n-3 DPA", "EPA", "AA") # Update in case you want to create other models.
# The loop goes through all the elements in gropus and creates a model for each of them. The model can not work with
# only one row, so the "if" makes sure that only the subgroups with more than one row are analyzed.
for (lm in 1:length(groups)) {
if (nrow(groups[[lm]]) > 1) {
# Training data set:
transpose <- matrix(as.numeric(t(groups[[lm]])),nrow=nrow(t(groups[[lm]]))) # Transpose and create matrix.
rownames(transpose) <- rownames(t(groups[[lm]]))
colnames(transpose) <- colnames(t(groups[[lm]]))
scale <- scale(transpose, center = FALSE, scale = TRUE) # All the model will use scale data
# Validation data set:
val_transpose <- matrix(as.numeric(t(val_groups[[lm]])),nrow=nrow(t(val_groups[[lm]]))) # Transpose and create matrix.
rownames(val_transpose) <- rownames(t(val_groups[[lm]]))
colnames(val_transpose) <- colnames(t(val_groups[[lm]]))
val_scale <- scale(val_transpose, center = FALSE, scale = TRUE) # All the model will use scale data
# Support Vector Machine Model:
support_vm <- cfBuild(scale, response_matrix, bootNum = 65, ensNum = 65, cpus = 4) # Creates model
# Validation of the models:
pred_val <- cfPredict(support_vm, val_scale)
names(pred_val) <- c("prediction")
mcc_val = mcc(preds = pred_val$prediction, actuals = val_response$responses)
average_right <- data.frame(groups = names(groups)[[lm]],
percentage_accuracy = getAvgAcc(support_vm)$Test,
MCC= mcc_val) # Save %CC value.
accuracy_table <- rbind(accuracy_table, average_right) # Append %CC and MCC to the accuracy table.
# Save model as a R object.
saveRDS(support_vm,
file = paste(output, "1_SVM_",
names(groups)[[lm]], ".R", sep = ""),
ascii = FALSE, version = NULL, compress = TRUE, refhook = NULL)
}
else { next }
}
# OUTPUT:
# Besides the functional models, the OUTPUT of this script is a table with Accuracy score of the model and the
# validation of the model based on the MCC value.
write.table(accuracy_table,
file = paste(output, "1_accuracy_table.txt", sep = ""),
sep = "\t",
quote = FALSE,
row.names = FALSE)
#------------------------------------------ RANDOMFOREST LIPIDOMIC DATA ------------------------------------------#
# Rheumatoid arthritis is a inflammatory disease that characterized for not having a resolution phase in the
# inflammatory procces. One of the most used treatments is modifying antirheumatic drugs (DMARDs), that is not
# always efective.
# We have a dataset of the lipid profiles of rheumatoid arthritis patients that responded or not to the treatment.
# This is the first approach to creat a model using "randomForest" (decision trees).
#---> LIBRARY LOAD:
library(randomForest)
library("mltools")
set.seed(415) # To get same results even with the random part.
#---> INPUT AND OUTPUT:
# In this section please specify where are the input files and where you want to save the output files.
# In the input and output variable you can see what is the path expected from the user to write.
input <- "C:/Users/hhy270/Documents/GitHub/2018_Machine_Learning_MTX_treatment_in_RA_patients/a_Toy_Data/2_randomForest_(RF_models)/"
output <- "C:/Users/hhy270/Documents/GitHub/2018_Machine_Learning_MTX_treatment_in_RA_patients/c_Expected_Output/2_randomForest_(RF_models)/"
# !!!! IMPORTANT: For this script to work the training dataset has to be called: 2_randomForest_(RF_models)_toy_data.txt
# !!!! IMPORTANT: For this script to work the validation dataset has to be called: 2_randomForest_(RF_models)_data_validation.txt
#---> DATA MANIPULATION:
# TRAINING SET:
# Data uses to create the model!
# Open the txt file with the profiles information. Make sure that the path is correct:
# The dataset consist in a tab-delimited file in .txt format with the follow specifications:
# Columns: The different lipid mediators plus a column called "responses" that contains information about the
# "Responder" and "Non_Responder".
# Rows: The different samples (each patient data).
# See a_Toy_Data/2_randomForest_(RF_models)/2_randomForest_(RF_models)_toy_data.txt
lm_profiles <- read.table(
file = paste(input, "2_randomForest_(RF_models)_toy_data.txt", sep = ""),
header = TRUE,
row.names = 1, # Specify that the first column is row names.
sep = "\t")
lm_profiles_scale <- as.data.frame(scale(lm_profiles[, -1], center = FALSE, scale = TRUE))
# Add the classification variable to the data frame (Responder and non responder):
# Getting the explanatory (x) and response (y) variable. By explanatory, it means all the data that can explain why a
# patient response or not to the treatment (the lipid meadiator profiles) and the response variable is if the
# patients response or not to the treatment. In random Forest you have to create a formula where the response
# variable is explain in terms of the explanatory variable (responses ~ everything else).
lm_profiles_scale$responses <- lm_profiles$responses
# Make sure that column names do not represent a problem to randomForest making them a valid name to R.
names(lm_profiles_scale) <- make.names(names(lm_profiles_scale))
# Separates the profiles data by lipid mediators types.
# By Substrates:
dha <- lm_profiles_scale[ ,c(1:24, 56)]
n_three_DPA <- lm_profiles_scale[ , c(25:34, 56)]
epa <- lm_profiles_scale[ , c(35:37, 56)]
aa <- lm_profiles_scale[ , c(38:55, 56)]
# VALIDATION SET:
# Data use to test the model (independent cohort)!
# The dataset consist in a tab-delimited file in .txt format with the follow specifications:
# Columns: The different lipid mediators plus a column called "responses" that contains information about the
# "Responder" and "Non_Responder".
# Rows: The different samples (each patient data).
val_lm_profiles <- read.table(
file = paste(input, "2_randomForest_(RF_models)_data_validation.txt", sep = ""),
header = TRUE,
row.names = 1, # Specify that the first column is row names.
sep = "\t")
val_lm_profiles_scale <- as.data.frame(scale(val_lm_profiles[, -1], center = FALSE, scale = TRUE))
names(val_lm_profiles_scale) <- make.names(names(val_lm_profiles_scale))
# By Substrates:
val_dha <- val_lm_profiles_scale[ ,c(1:24)]
val_n_three_DPA <- val_lm_profiles_scale[ , c(25:34)]
val_epa <- val_lm_profiles_scale[ , c(35:37)]
val_aa <- val_lm_profiles_scale[ , c(38:55)]
#---> MACHINE LEARNING (randomForest R):
# In Random Forests the idea is to decorrelate the several trees which are generated on the different bootstrapped
# samples from training Data and then reduce the variance in the trees by averaging them.
# Averaging the trees also improve the perfomance of decision trees on Test Set and eventually avoid overfitting.
# The idea is to build lots of trees in such a way to make the correlation between the trees smaller.
# BEST MTRY:
# mtry is the number of variables available for splitting at each tree node. Random Forest creates several trees,
# each one using different variables to create the best version of it. With mtry we can define how many variables
# the data is split to create the different trees.
# More: https://stats.stackexchange.com/questions/102867/random-forest-mtry-question
# In this case we defined the lipid mediators to create a loop to define which mtry is the best one for our model.
oob_error <- double(ncol(lm_profiles_scale) - 1) #Define number of variable. -1 is because the last column is responses.
# Loop to select the best mtry.
for (mtry in 1:(ncol(lm_profiles_scale) - 1)) {
# NOTE:
# importance = TRUE creates the plot of the important variables, that can gave us an idea, based on the
# decrease of the accuracy of the models, what lipid mediators are contributing to make a better model.
rf_lm_profiles_scales <- randomForest(responses ~ ., data = lm_profiles_scale, mtry = mtry,
importance = TRUE, ntree = 10000)
oob_error[mtry] <- 100 - ((rf_lm_profiles_scales$err.rate[10000])*100)
}
# Define the best mtry according to the best prediction value.
final_mtry <- which.max(oob_error)
# Run the model again with the right mtry value.
rf_lm_profiles_final <- randomForest(responses ~ ., data = lm_profiles_scale, mtry = final_mtry,
importance = TRUE, ntree = 10000)
# Save relevant Plots:
pdf(file = paste(output, "2_RF_lmprofiles_scale.pdf", sep = ""),
width = 14, height = 10, onefile = TRUE)
varImpPlot(rf_lm_profiles_final, sort = TRUE, main = "lm_profiles") # Importance of the variable for the model.
plot(rf_lm_profiles_final, main = "lm_profiles") # Decreasing of the error base on the number of tres.
legend("topright", inset=.05, title="Curves:",
c("Non responder", "OOB error", "Responder"), fill=c("red", "black", "green"),
text.font = 3, cex = 1)
dev.off()
#---> VALIDATION TEST:
# "prediction" takes the created models and the validation dataset to try to predict which samples belongs to the
# responder and non-responder.
pred_rf_lm_profiles <- as.data.frame(predict(rf_lm_profiles_final, val_lm_profiles_scale))
names(pred_rf_lm_profiles) <- c("prediction")
# In order to further evaluate the predictivenss of this approach we next calculated  Matthews correlation
# coefficient (MCC), which represents the accuracy of the model at predicting outcome. Very helpful when you
# have imbalance data.
mcc_value = mcc(preds = pred_rf_lm_profiles$prediction, actuals = val_lm_profiles$responses) # From the mltools package.
# Creates a table with all the models, the %CC and the MCC.
# An error estimate is made for the cases which were not used while building the tree. That is called an
# OOB (Out-of-bag) error estimate which is mentioned as a percentage.
# In this case, then, if a model has 1% error will means that we can predict with 99% accuracy.
no_oob_error_table <- data.frame(groups = "scalated lm profiles",
percentage_accuracy = 100 - ((rf_lm_profiles_final$err.rate[10000])*100),
MCC = mcc_value,
stringsAsFactors = FALSE)
# Save the models as an R object:
# To avoid running all the scripts to obtain the models, it can be saved as R objects. If you want to use it,
# you can call it using "readRDS".
saveRDS(rf_lm_profiles_final,
file = paste(output, "2_RF_lmprofiles_scale.R", sep = ""),
ascii = FALSE, version = NULL, compress = TRUE, refhook = NULL)
#---> RF PER GROUP:
# Create a list with the names of all the subgroups:
# Modifying the original lipid mediator file, including other data (such as clinical scores) and dividing the table
# in different sections is possible to create automatically all the models you want. YOU NEED TO MAKE SURE WHATSOEVER
# that the "groups" list and the names of the "groups" list is updated.
groups <- list(dha, n_three_DPA, epa, aa) # Update in case you want to create other models.
val_groups <- list(val_dha, val_n_three_DPA, val_epa, val_aa) # Update in case you want to create other models.
# Create a vector with the names associated to all the elements in the list:
names(groups) <- c("DHA", "n-3 DPA", "EPA", "AA") # Update in case you want to create other models.
# The loop goes through all the elements in gropus and creates a model for each of them. The model can not work with
# only one row, so the "if" makes sure that only the subgroups with more than one row are analyzed.
for (lm in 1:length(groups)) {
if (ncol(groups[[lm]]) > 2) {
# Identifying the best mtry:
oob_error_all <- double(ncol(groups[[lm]]) - 1)
for (mtry_all in 1:(ncol(groups[[lm]]) - 1)) {
random_forest <- randomForest(responses ~ ., data = groups[[lm]], mtry = mtry_all,
importance = TRUE, ntree = 10000)
oob_error_all[mtry_all] <- 100 - ((random_forest$err.rate[10000])*100)
}
# Creating the final model:
final_mtry_all <- which.max(oob_error_all)
random_forest_final <- randomForest(responses ~ ., data = groups[[lm]], mtry = final_mtry_all,
importance = TRUE, ntree = 10000)
# Model Graphs:
pdf(file = paste(output, "2_RF_",
names(groups)[[lm]], ".pdf", sep = ""),
width = 14, height = 10, onefile = TRUE)
varImpPlot(random_forest_final, sort = TRUE, main = names(groups)[[lm]]) # Importance of the variable for the model.
plot(random_forest_final, main = names(groups)[[lm]]) # Decreasing of the error base on the number of tres.
legend("topright", inset=.05, title="Curves:",
c("Non responder", "OOB error", "Responder"), fill=c("red", "black", "green"),
text.font = 3, cex = 1)
dev.off()
# Validation:
pred_val <- as.data.frame(predict(random_forest_final, val_groups[[lm]]))
names(pred_val) <- c("prediction")
mcc_val = mcc(preds = pred_val$prediction, actuals = val_lm_profiles$responses)
# Accuracy table:
accuracy_table <- data.frame(groups = names(groups)[[lm]],
percentage_accuracy = 100 - ((random_forest_final$err.rate[10000])*100),
MCC = mcc_val,
stringsAsFactors = FALSE)
no_oob_error_table <- rbind(no_oob_error_table, accuracy_table)
# Save final model:
saveRDS(random_forest_final,
file = paste(output, "2_RF_",
names(groups)[[lm]], ".R", sep = ""),
ascii = FALSE, version = NULL, compress = TRUE, refhook = NULL)
}
else { next }
}
# OUTPUT:
# Besides the functional models, the OUTPUT of this script is a table with Accuracy score of the model and the
# validation of the model based on the MCC value.
# The resulting plots also gave us an idea of the performance of the models, that includes the "importance" analysis,
# and the improvement of the model based on the number of tress used to create it.
write.table(no_oob_error_table,
file = paste(output, "2_accuracy_table_RF.txt", sep = ""),
sep = "\t",
quote = FALSE,
row.names = FALSE)
#---------------------------------- DIFFERENTIAL GENE EXPRESSION ANALYSIS  ----------------------------------------#
# Rheumatoid arthritis is a inflammatory disease that characterized for not having a resolution phase in the
# inflammatory procces. One of the most used treatments is modifying antirheumatic drugs (DMARDs), that is not
# always efective.
# A dataset of Gene Expression level (READ COUNTS) of meaningful enzymes in the lipid mediator pathways of
# Responder and Non Responder Patients is the input of this script along with a classification table of patients.
# This is the first approach to analysis of this data.
#---> LIBRARY LOAD:
library(edgeR)
library(ggplot2)
#---> INPUT AND OUTPUT:
# In this section please specify where are the input files and where you want to save the output files.
# In the input and output variable you can see what is the path expected from the user to write.
input <- "C:/Users/hhy270/Documents/GitHub/2018_Machine_Learning_MTX_treatment_in_RA_patients/a_Toy_Data/3_DGE_analysis_(Edge_R)/"
output <- "C:/Users/hhy270/Documents/GitHub/2018_Machine_Learning_MTX_treatment_in_RA_patients/c_Expected_Output/3_DGE_analysis_(Edge_R)/"
# !!!! IMPORTANT: For this script to work the training dataset has to be called: 3_DGE_analysis_(Edge_R)_read_counts.txt
# !!!! IMPORTANT: For this script to work the validation dataset has to be called: 3_DGE_analysis_(Edge_R)_class_reads.txt
#---> DATA LOAD:
# Open the txt file with the gene expression information (read counts).
# The dataset consist in a tab-delimited file in .txt format with the follow specifications:
# Columns: The different samples (each patient data).
# Rows: The interested enzyme read counts for each patient.
# See a_Toy_Data/3_DGE_analysis_(Edge_R)/3_DGE_analysis_(Edge_R)_read_counts.txt
counts_blood <- read.table(
file = paste(input, "3_DGE_analysis_(Edge_R)_read_counts.txt", sep = ""),
header = TRUE,
row.names = 1, # Specify that the first column is row names.
sep = "\t")
# Open the txt file with the classification table (Responder and Non Responder).
# The table consist in two columns: column "Samples" with the different sample IDs and the column "Response"
# with information for each patient about whether they are responders or not.
# See a_Toy_Data/3_DGE_analysis_(Edge_R)/3_DGE_analysis_(Edge_R)_class_reads.txt
classification_blood <- read.table(
file = paste(input, "3_DGE_analysis_(Edge_R)_class_reads.txt", sep = ""),
header = TRUE,
sep = "\t")
#---> DATA PREPARATION:
# Define the response to the treatment as a factor elements and associate them with the samples names.
# This will be helpful at the moment to design the model who is going to identify statistical differents
# between the two groups.
classes_blood <- as.factor(classification_blood$Response)
names(classes_blood) <- names(counts_blood)
#---> GENE EXPRESSION ANALYSIS (EDGE R):
# Edge R does differential expression analysis of RNA-seq expression profiles with biological replication.
# Implements a range of statistical methodology based on the negative binomial distributions, including empirical
# Bayes estimation, exact tests, generalized linear models and quasi-likelihood tests.
# It takes raw read counts to performance the analysis so pre-normalization steps are not requiered.
DGE_file_blood <- DGEList(counts = counts_blood, group = classes_blood) # Creates the DGE file.
# Gene to be expressed at a reasonable level in a sample if two counts per each million mapped reads in that sample.
# Gene should be expressed in at least one condition.
keep <- rowSums(cpm(DGE_file_blood)>2) >= 10
DGE_file_blood <- DGE_file_blood[keep, , keep.lib.sizes = FALSE]
DGE_file_blood <- calcNormFactors(DGE_file_blood) # TMM Normalization
# Since it is not a paired comparison and there is not a batch effect, the comparison design is only based on the
# groups.
design_edge_blood <- model.matrix(~classes_blood)
# Estimate the dispersion. robust = TRUE -> Robustified against potential outlier genes.
DGE_file_blood_d <- estimateDisp(DGE_file_blood, design_edge_blood,  robust = TRUE)
# The quasi-likelihood method was used to calculated differences between the groups/
fit_blood_r <- glmQLFit(DGE_file_blood_d, design_edge_blood)
lrt_blood <- glmQLFTest(fit_blood_r, coef = 2) # Coef = 2 specify comparison Non Responder vs Responder.
# The results of the DGE analysis can be seen as a data frame tha cointains for each gene the next information:
# LogFC (Log(FoldChange)).
# LogCPM (Log(CountPerMillion)).
# F value.
# p value and adjust p value (FDR), in this case using the BH correction.
edge_fc_table_blood <- as.data.frame(topTags(lrt_blood, n = Inf, adjust.method = "BH", sort.by = "PValue"))
# The Output of this section is a tab-delimited table with the information of DGE results:
write.table(edge_fc_table_blood,
file = paste(output, "3_DGE_results.txt", sep = ""),
sep = "\t",
quote = FALSE,
row.names = TRUE)
# ---> VIOLIN PLOTS:
# This section creates Violin plots for the Alox related genes (ALOX12, ALOX15, ALOX15B and ALOX5).
# Get only the interested genes.
counts_blood <- counts_blood[c(2:5), ]
# Calculates the Log(cpm) for each gene:
immune_reads <- cpm(counts_blood)
immune_reads_log <- cpm(counts_blood, log =TRUE)
immune_reads_log <- as.data.frame(t(immune_reads_log))
immune_reads <- as.data.frame(t(immune_reads))
immune_reads$groups <- classification_blood$Response
# Create a data frame that will contains the requieres information for the violin plots:
violin_table <- data.frame(groups = rep(classification_blood$Response, 4))
genes <- NULL
counts <- NULL
# Small loop that get the information from the counts_genes data frame to the violin_table data frame:
for (i in 1:ncol(immune_reads_log)) {
genes[i] <- list(rep(colnames(immune_reads_log)[i], ncol(counts_blood)))
counts[i] <- list(immune_reads_log[, i])
}
violin_table$genes <- unlist(genes)
violin_table$counts <- unlist(counts)
# Specify the factors of the comparison to put them in the violin plot:
violin_table$genes <- factor(violin_table$genes, levels = colnames(immune_reads_log))
violin_table$groups <- factor(violin_table$groups, levels = c("responder", "non_responder"))
# VIOLIN PLOTS:
# Creates and saves as a pdf the violin plots:
pdf(file = paste(output, "3_Violin_plot_ALOX.pdf", sep = ""),
width = 25, height = 12, onefile = TRUE)
ggplot(violin_table, aes(y = counts, x = groups)) +
geom_violin(aes(fill = groups), trim = FALSE) +
scale_fill_manual(values = c("lightgreen", "firebrick2")) +
facet_wrap("genes", scales = "free_y", ncol = 2) +
scale_y_continuous(name = "Log(CPM)") +
labs(x = "Response to treatment") +
theme(plot.title = element_text(size = 30, hjust = 0.5),
axis.title = element_text(size = 30),
axis.text.x  = element_text(size = 20, hjust = 0.5),
axis.text.y  = element_text(size = 30, hjust = 1),
legend.title = element_text(size = 20),
strip.text.x = element_text(size = 25),
legend.text  = element_text(size = 20),
legend.position = "none",
panel.grid.major = element_blank())
dev.off()
